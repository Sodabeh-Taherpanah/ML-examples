{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "korean-wednesday",
      "metadata": {
        "id": "korean-wednesday"
      },
      "source": [
        "# Support Vector Machines\n",
        "You should build a machine learning pipeline using a support vector machine model. In particular, you should do the following:\n",
        "- Load the `mnist` dataset using [Pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). You can find this dataset in the datasets folder.\n",
        "- Split the dataset into training and test sets using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "- Conduct data exploration, data preprocessing, and feature engineering if necessary.\n",
        "- Train and test a support vector machine model using [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "- Check the documentation to identify the most important hyperparameters, attributes, and methods of the model. Use them in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*.   Loading the dataset (MNIST) using Pandas.  \n",
        "*.   Splitting the data into training and test sets.\n",
        "*.   Exploratory Data Analysis (EDA) and preprocessing.\n",
        "*.   Training an SVM model using Scikit-Learn.\n",
        "*.   Evaluating the model's performance.\n",
        "*.   Tuning hyperparameters to improve accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I3fshwJqRK1E"
      },
      "id": "I3fshwJqRK1E"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "infrared-copper",
      "metadata": {
        "id": "infrared-copper"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset (Assuming it's in CSV format)\n",
        "mnist = pd.read_csv(\"datasets/mnist.csv\")\n",
        "\n",
        "# Display dataset information\n",
        "print(mnist.info())\n",
        "print(mnist.head())\n",
        "\n",
        "# Check if there are missing values\n",
        "print(mnist.isnull().sum().sum())  # Should be 0"
      ],
      "metadata": {
        "id": "7gmie4m5SsB_"
      },
      "id": "7gmie4m5SsB_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Separate features and target\n",
        "X = mnist.drop(columns=['label'])  # Features (pixel values)\n",
        "y = mnist['label']  # Target variable (digit)\n",
        "\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "qvFTTzhfSuxf"
      },
      "id": "qvFTTzhfSuxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show some sample images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = X_train.iloc[i].values.reshape(28, 28)  # Reshape 784 pixels into 28x28 image\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f\"Label: {y_train.iloc[i]}\")\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jRI0C7GMS5ZC"
      },
      "id": "jRI0C7GMS5ZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " SVM is sensitive to feature scales, we normalize pixel values. -->  (0 to 255) using StandardScaler."
      ],
      "metadata": {
        "id": "WMZkdRgzTCk8"
      },
      "id": "WMZkdRgzTCk8"
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "lLuIDVdNTBkR"
      },
      "id": "lLuIDVdNTBkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the SVM Model"
      ],
      "metadata": {
        "id": "3LU2DLuMUL1d"
      },
      "id": "3LU2DLuMUL1d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SVM model with RBF kernel\n",
        "svm_model = SVC(kernel='rbf', C=10, gamma=0.01, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "516qB5d4VPoK"
      },
      "id": "516qB5d4VPoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M--XQD0vTjBj"
      },
      "id": "M--XQD0vTjBj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the Best Hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "zNK7LVbmUgxT"
      },
      "id": "zNK7LVbmUgxT"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [1, 10, 100],\n",
        "    'gamma': [0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train final model with best parameters\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_best = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# Final accuracy\n",
        "print(f\"Optimized Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n"
      ],
      "metadata": {
        "id": "L9jBPX_eUU0u"
      },
      "id": "L9jBPX_eUU0u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}